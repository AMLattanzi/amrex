\section{What is \BoxLib?}

\BoxLib\ is a software library containing all the functionality to write parallel, 
block-structured adaptive mesh refinement (AMR) applications.
\BoxLib\ was developed at the Center for Computational Sciences and Engineering (CCSE) at 
Lawrence Berkeley National Laboratory.  Further information can be found by contacting 
Mike Lijewski of CCSE at {\tt MJLijewski@lbl.gov} or by visiting our webpage
at {\tt https://ccse.lbl.gov/}.  Key features of \BoxLib\ include:

\begin{itemize}
\item Support for block-structured AMR with optional subcycling in time
\item Support for cell-centered, face-centered, and nodal data
\item Support for hyperbolic, parabolic, and elliptic solves on hierarchical grid structure
\item C++ and Fortran90 versions
\item Supports hybrid programming model with MPI and OpenMP
\item Basis of mature applications in combustion, astrophysics, cosmology, and porous media
\item Demonstrated scaling of linear solvers to 100,000 processors and 
      hydrodynamics to over 200,000 processors
\item Freely available to interested users on our website
\item The plotfile format generated by \BoxLib\ can be read by {\tt VisIt}, {\tt AmrVis},
      and {\tt yt}.
\end{itemize}

\section{Parallel Programming Model}

The fundamental parallel abstraction in \BoxLib\ is the \MultiFab, which holds the data on the 
union of grids at a level of refinement.  A \MultiFab\ is composed of ``Fortran array boxes''
(i.e., \FArrayBox es or \Fab s); each \Fab\ is an array of data on a single grid. 
Whenever ``work'' needs to be done using data from a \MultiFab, the 
\Fab s composing that \MultiFab\ are distributed 
among the cores.  \MultiFab s at each level of refinement are distributed 
independently.  The software supports two data distribution schemes, as well as a 
dynamic switching scheme that decides which approach to use based on the number of 
grids at a level and the number of processors.  The first scheme is based on a 
heuristic knapsack algorithm, which focuses on load balancing; the second is based on 
the use of a Morton-ordering space-filling curve, which focuses on data locality. 
 \MultiFab\ operations are performed with an ``owner computes'' rule 
with each processor operating independently on its local data.  For operations that 
require data owned by other processors, the \MultiFab\ operations are preceded by a 
data exchange between processors to fill ghost cells.  Each processor contains 
meta-data that is needed 
to fully specify the data locality and processor assignments of the \Fab s. At a 
minimum, this requires the storage of an array of coordinates specifying the index space 
region for each box at AMR level of refinement.  The meta-data can thus be used to 
dynamically evaluate the necessary communication patterns for sharing data amongst 
processors, enabling us to optimize communications patterns within the algorithm.
One of the advantages of computing with fewer, larger grids in the hybrid 
MPI-OpenMP approach (see below) is that the size of the meta-data is substantially 
reduced.

\subsection{Hybrid MPI--OpenMP}

The basic parallelization strategy uses a hierarchical programming approach for 
multicore architectures based on both MPI and OpenMP.  In the pure-MPI instantiation, 
each \Fab\ is is assigned to a core, and each core communicates 
with every other core using only MPI.  In the hybrid approach, where on each socket/node 
there are $n$ cores which all access the same memory, we can divide our domain into
fewer, larger grids, and assign each \Fab\ to a socket/node, 
with the work associated with that grid distributed among the $n$ 
cores using OpenMP.

\subsection{Parallel I/O}

Data for checkpoints and analysis are written in a self-describing format that consists 
of a directory for each time step written. Checkpoint directories contain all necessary 
data to restart the calculation from that time step. Plotfile directories contain data 
for postprocessing, visualization, and analytics, which can be read using {\tt VisIt}, {\tt yt}, or
{\tt AmrVis}, a customized visualization package developed at CCSE for visualizing 
data on AMR grids.  Within each checkpoint or plotfile directory is an ASCII header file and 
subdirectories for each AMR level.  The header describes the AMR hierarchy, including 
number of levels, the grids at each level, the problem size, refinement ratio 
between levels, time step, time, etc.  Each of the subdirectories contains the data 
associated with the \MultiFab\ for that level, which is stored in multiple files.
Checkpoint and plotfile directories are written at user-specified intervals. 

Restarting a calculation can present some difficult issues for reading data efficiently. 
In the worst case, all processors would need data from all files. If multiple processors 
try to read from the same file at the same time, performance problems can result, with 
extreme cases causing file system thrashing. Since the number of files is generally not 
equal to the number of processors and each processor may need data from multiple files, 
input during restart is coordinated to efficiently read the data. Each data file is only 
opened by one processor at a time. The IOProcessor creates a database for mapping files 
to processors, coordinates the read queues, and interleaves reading its own data. Each 
processor reads all data it needs from the file it currently has open. The code tries to 
maintain the number of input streams to be equal to the number of files at all times. 
Checkpoint and plotfiles are portable to machines with a different byte ordering and 
precision from the machine that wrote the files. Byte order and precision translations 
are done automatically, if required, when the data is read.

\subsection{Scaling}

In Figure \ref{fig:scaling} we present weak scaling results for several of our codes on 
the Cray XT5 Jaguarpf at OLCF. Jaguarpf has two hex-core sockets on each node. We assign 
one MPI process per node and spawn a single thread on each of the 12 cores. Results are 
shown for our compressible astrophysics code, {\tt CASTRO}; the low Mach number code, 
{\tt MAESTRO}; and our low Mach number combustion code, {\tt LMC}. In the {\tt MAESTRO} 
and {\tt CASTRO} tests, we simulate a full spherical star on a 3D grid with one refined 
level (2 total levels).  {\tt LMC} is tested on a 3D methane flame with detailed chemistry 
using two refined levels. {\tt MAESTRO} and {\tt LMC} scale well to 50K-100K cores, 
whereas {\tt CASTRO} scales well to over 200K cores. The overall scaling behavior 
for {\tt MAESTRO} and {\tt LMC} is not as close to ideal as that of {\tt CASTRO} 
due to the communication-intensive linear solves performed at each time step. However, 
these low Mach number codes are able to take a much larger time step than explicit 
compressible formulations in the low Mach number regime. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[tb]
\centering
\includegraphics[width=3in]{./Introduction/castro_scaling}
\includegraphics[width=3in]{./Introduction/maestro_scaling}
\includegraphics[width=3in]{./Introduction/lmc_scaling}
\caption{\label{fig:scaling}Weak scaling results for {\tt CASTRO}, {\tt MAESTRO}, and
{\tt LMC} on the Cray XT5 Jaguarpf at OLCF.}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Overview of Data Structures}

\BoxLib\ contains the most fundamental objects used to construct parallel
block-structured AMR applications.
At each level of refinement, the region covered by that level is divided
into grids, or boxes.  The entire computational domain is covered by
the coarsest (base) level of refinement, often called level $\ell=0$. 
Higher levels of refinement have cells that are finer by a ``refinement ratio''
of either 2 or 4.  The grids are properly nested in the sense that the union 
of grids at level $\ell+1$ is contained in the union of grids at level $\ell$.
Furthermore, the containment is strict in the sense that, except at physical 
boundaries, the level $\ell$ grids are large enough to guarantee that there is
a border at least $n_{\rm buffer}$ level $\ell$ cells wide surrounding each level
$\ell +1$ grid (grids at all levels are allowed to extend to the physical
boundaries so the proper nesting is not strict there).  See Figure \ref{fig:AMR}
for a sample two-dimensional grid structure.
As discussed earlier, the grids are distributed to processors in
a fashion designed to put roughly equal amounts of work on each
processor (load balancing).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[tb]
\centering
\includegraphics[width=6in]{./Introduction/AMR}
\caption{\label{fig:AMR}Sample grid structure with two levels of refinement.  These
grids satisfy the requirements that the base grid covers the entire computational domain 
and the grids are properly nested.  Note that refined grids are allowed to extend to the
domain boundary without coarser buffer cells.}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

On a grid, the data can be stored at cell-centers, faces, edges, or
corners (often called nodes).  In \BoxLib, data that is on an face is termed `nodal'
in that direction (see Figure~\ref{fig:dataloc}).  Data that is on the
corners is nodal in all spatial directions.  In three-dimensional problems, data
that is nodal in exactly two directions lies on edges.
\BoxLib\ uses $0$-based indexing, and for nodal data, the integer index
corresponds to the lower boundary in that direction.
In our \BoxLib\ applications, the state data (velocity, density, 
species, $\ldots$) is generally cell-centered.  Fluxes are typically nodal a
single direction.  A few quantities are nodal in all directions 
(e.g.~the pressure in the low Mach number projection methods).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[tb]
\centering
\includegraphics[width=6.5in]{./Introduction/data_loc2}
\caption{\label{fig:dataloc} Some of the different data-centerings in two dimensions:
(a) cell-centered, (b) nodal in the $x$-direction only, and (c) nodal in
both the $x$- and $y$-directions.  Note that for nodal data, the
integer index corresponds to the lower boundary in that direction.
In each of these centerings, the red point has the same indices:\ (1,2).
Not shown is the case where data is nodal in the $y$-direction only.  
Also note that \BoxLib\ uses $0$-based indexing.}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Note that when a C++ \BoxLib\ application is compiled and linked,
the number of spatial dimensions (1, 2, or 3), {\tt DIM},
of the code must be specified.  The code that will be
built is specifically designed to run only with that number of dimensions.
This is unlike the Fortran90 \BoxLib\ data structures in which we build
dimension-independent code at compile-time.

To simplify the description of the underlying AMR grid, \BoxLib\
provides a number of classes.  We now briefly summarize some of the major
classes.

\subsection{\IntVect}

\IntVect s are {\tt DIM}-tuples of integers that are used to define
indices in space.  In C++ \BoxLib\, an example of an \IntVect\ in 2D would be

\begin{lstlisting}[language={[gnu]make},mathescape=false]
IntVect iv(3,5);
\end{lstlisting}
In Fortran90 \BoxLib\, we don't use \IntVect\ s, but instead use standard
arrays of integers:
\begin{lstlisting}[language={[gnu]make},mathescape=false]
integer :: iv(2)
iv(1) = 3
iv(2) = 5
\end{lstlisting}

\subsection{\BoxType}

A \BoxType\ is simply a rectangular domain in space and does not hold any data.
A \BoxType\ contains the indices of its low end and high end, 
{\tt IntVect lo} and {\tt IntVect hi}.  In C++ \BoxLib, a \BoxType\ also
contains an {\tt IndexType} (cell-centered, face-centered, or nodal) for each
dimension.  In Fortran90 \BoxLib, a \BoxType\ also contains the dimensionality 
of the \BoxType.  To build a \BoxType\ in C++ \BoxLib\ use:
\begin{lstlisting}[language={[gnu]make},mathescape=false]
IntVect iv_lo(0,0);
IntVect iv_hi(15,15);
Box bx(iv_lo,iv_hi);
\end{lstlisting}
To build a \BoxType\ in Fortran90 \BoxLib\ use:
\begin{lstlisting}[language={[gnu]make},mathescape=false]
type(box) :: bx
integer   :: iv_lo(2), iv_hi(2)
iv_lo(1:2) = 0
iv_hi(1:2) = 15
bx = make_box(lo,hi)
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[tb]
\centering
\includegraphics[width=4.0in]{./Introduction/index_grid2}
\caption{\label{fig:boxes} Three boxes that comprise a single level.
At this resolution, the domain is 16$\times$16 cells.}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The computational domain is divided into non-overlapping grids.  The collection of
grids at the same resolution comprise a level.
Figure~\ref{fig:boxes} shows three grids at the same level of
refinement.  The position of the grids is with respect to a global
index space at that level.  For example, the \BoxType\ associated with grid 1 
in the figure has {\tt lo} = (2,6) and {\tt hi} = (5,13).  In C++ \BoxLib, for a 
\BoxType\ {\tt bx}, you can access the indices using
\begin{lstlisting}[language={[gnu]make},mathescape=false]
IntVect iv_lo = bx.smallEnd(); // for grid 1 above this would return (2,6)
IntVect iv_hi = bx.bigEnd(); // for grid 1 above this would return (5,13)
\end{lstlisting}
In Fortran90 \BoxLib, for a 
\BoxType\ {\tt bx}, you can access the indices using
\begin{lstlisting}[language={[gnu]make},mathescape=false]
integer :: iv_lo(2), iv_hi(2)
iv_lo(1:2) = bx%lo(1:2)  ! for grid 1 above this would return (2,6)
iv_hi(1:2) = bx%hi(1:2)  ! for grid 1 above this would return (5,13)
\end{lstlisting}

\subsection{\BoxArray}

A \BoxArray\ is an array of \BoxType es.   The size of the array is the 
number of \BoxType es in the \BoxArray.
Suppose you have a \BoxType\ {\tt bx} with lo indices (0,0) and hi indices (15,15).
In C++ \BoxLib, you could first initialize a \BoxArray\ to have that single box, and then
chop up the domain so that your \BoxArray\ contains 4 8$\times$ 8 \BoxType\ es using:
\begin{lstlisting}[language={[gnu]make},mathescape=false]
BoxArray bx(domain);
ba.maxSize(8);
\end{lstlisting}
The analogous code in Fortran90 \BoxLib\ is
\begin{lstlisting}[language={[gnu]make},mathescape=false]
type(boxarray) :: ba
call boxarray_build_bx(ba,bx)
call boxarray_maxsize(ba,8)
\end{lstlisting}

\subsection{\Layout\ (Fortran90 Only)}

A \Layout\ is a more intelligent \BoxArray, since it contains a \BoxArray\ as well
as the associated processor assignments, \BoxType\ connectivity, and many other
parallel constructs.  In the simplest case, if we have a \BoxArray\ {\tt ba}, a 
\Layout\ can be defined using:
\begin{lstlisting}[language={[gnu]make},mathescape=false]
type(layout) :: la
call layout_build_ba(la,ba)
\end{lstlisting}
In C++ \BoxLib, the information that is contained in the Fortran90 \Layout\ part of
the \MultiFab\ class.

\subsection{\FArrayBox}

A \FArrayBox\ (or \Fab) is a ``Fortran array box'' that holds data.  It contains the
\BoxType\ that it is built on as well as a pointer to the data 
that can be sent to a Fortran routine.  In \BoxLib, we don't usually deal with 
\Fab s alone, but rather through \MultiFab s, described next.

\subsection{\MultiFab}

A \MultiFab\ is a collection of all the \Fab s at the same level of
refinement.  In C++ \BoxLib, a \MultiFab\ is defined using a \BoxArray,
number of components, and number of ghost cells that each \Fab\
will have.  In Fortran90 \BoxLib, a \MultiFab\ is defined using a \Layout,
number of components, and a number of ghost cells.
A \MultiFab\ has a ``valid'' region that is defined by 
the \BoxArray.  Each \Fab\ in the \MultiFab\ is built large enough 
to hold valid data and ghost cell data, and thus the \BoxType\ associated with
each \Fab\ is a grown version of the corresponding \BoxType\ from the \BoxArray.
Thus, a \Fab\ has no concept 
of ghost cells, it merely has a single \BoxType\ that identifies it.  
In C++ \BoxLib, if we have a \BoxArray {\tt ba}, then we could define a 
\MultiFab\ as:
\begin{lstlisting}[language={[gnu]make},mathescape=false]
MultiFab mf(ba,2,1);
\end{lstlisting}
In Fortran90 \BoxLib, if we have a \Layout\ {\tt la}, then we could define
a \MultiFab\ as:
\begin{lstlisting}[language={[gnu]make},mathescape=false]
type(multifab) :: mf
call multifab_build(mf,la,2,1)
\end{lstlisting}
In each case, the \MultiFab\ has two components of data and each \Fab\ in the \MultiFab\ contains 
ghost cells one row wide in all directions outside the box from the \BoxArray.

\section{\BoxLib\ Directory Structure}

\BoxLib\ is the base directory in a hierarchy of subdirectories that
support parallel, block-structured AMR applications in C++ and Fortran90.
A schematic of the \BoxLib\ directory structure is shown in Figure 
\ref{fig:boxlib_directory}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[tb]
\centering
\includegraphics[width=6.5in]{./Introduction/boxlib_directory_bw2}
\caption{\label{fig:boxlib_directory}\BoxLib\ directory structure.}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}

\item {\tt Docs/}

Contains the \BoxLib\ User's Guide.

\item {\tt Src/}

  \begin{itemize}

    \item {\tt C\_AMRLib/}
    \item {\tt C\_BaseLib/}
    \item {\tt C\_BoundaryLib/}
    \item {\tt F\_BaseLib/}

    Fortran90 \BoxLib\ source code.

    \item {\tt LinearSolvers/}

    \begin{itemize}

      \item {\tt C\_CellMG/}
      \item {\tt C\_NodalMG/}
      \item {\tt C\_TensorMG/}
      \item {\tt C\_to\_F\_MG/}
      \item {\tt F\_MG/}

      Fortran90 multigrid solver source code.

    \end{itemize}

  \end{itemize}

\item {\tt Tests/}

  \begin{itemize}

  \item {\tt C\_BaseLib/}
  \item {\tt F\_BaseLib/}
  \item {\tt LinearSolvers/}

  \end{itemize}

\item {\tt Tools/}

  \begin{itemize}

  \item {\tt C\_mk/}

  The generic Makefiles that store the C++ compilation flags for
  various platforms.

  \item {\tt C\_scripts/}

  Some simple scripts that are useful for building, running,
  maintaining codes in c++ \BoxLib.

  \end{itemize}

  \item {\tt C\_Util/}
  \item {\tt F\_mk/}

  The generic Makefiles that store the Fortran90 compilation flags for
  various platforms.

  \item {\tt F\_scripts/}

  Some simple scripts that are useful for building, running,
  maintaining codes in Fortran90 \BoxLib.

\item {\tt Tutorials/}

  Contains sample codes referred to in this User's Guide.

\end{itemize}
